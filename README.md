# ğŸ¤– Mistral Fine-tuned Model - Colab + Local Frontend

A Flask-based frontend server that connects to a Mistral model running on Google Colab. The model runs on Colab's GPU, and your local Flask server provides a beautiful web interface.

**Model:** [`KASHH-4/Mistral-Model-Legal-Advisor`](https://huggingface.co/KASHH-4/Mistral-Model-Legal-Advisor)

## âš ï¸ Important Security Notice

This project requires sensitive tokens and API URLs:
- **Never commit** your `.env` file (it's already in `.gitignore`)
- **Keep private:** Hugging Face tokens, ngrok tokens, and API URLs
- See [ğŸ” Security Notes](#-security-notes) section for details

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Browser   â”‚ â—„â”€â”€â”€â”€â”€â–º â”‚  Flask Server    â”‚ â—„â”€â”€â”€â”€â”€â–º â”‚  Colab Model    â”‚
â”‚  (Your Local)   â”‚         â”‚  (Your Local)    â”‚         â”‚  (Cloud GPU)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     index.html              app.py proxies                Mistral Model
     app.js                  requests to Colab             on GPU
     style.css
```

## ğŸ“ Project Structure

```
e:\EDI\hf-node-app\
â”œâ”€â”€ app.py                      # Flask server (proxies to Colab)
â”œâ”€â”€ colab_model_server.ipynb    # Run this on Google Colab
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ .env                        # Configuration (create this)
â””â”€â”€ static/
    â”œâ”€â”€ index.html              # Frontend UI
    â”œâ”€â”€ app.js                  # Frontend logic
    â””â”€â”€ style.css               # Styling
```

## ğŸš€ Setup Instructions

### Step 1: Setup Colab Model Server

1. Open `colab_model_server.ipynb` in Google Colab
2. Get your tokens:
   - **Hugging Face Token**: https://huggingface.co/settings/tokens
   - **ngrok Token**: https://dashboard.ngrok.com/get-started/your-authtoken
3. Run all cells in the notebook:
   - Install dependencies
   - Login to Hugging Face
   - Configure ngrok
   - Load the Mistral model
   - Start the Flask API server
4. **Copy the ngrok URL** (looks like: `https://xxxx-xx-xx-xx-xx.ngrok-free.app`)

### Step 2: Setup Local Frontend

1. **Create `.env` file** in the project root:
   ```bash
   # Copy the example file
   cp .env.example .env
   
   # Then edit .env and add your ngrok URL:
   API_URL=https://xxxx-xx-xx-xx-xx.ngrok-free.app
   PORT=7860
   ```
   
   **âš ï¸ Important:** Never commit your `.env` file - it's already in `.gitignore`

2. **Install Python dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the Flask server**:
   ```bash
   python app.py
   ```

4. **Open your browser**:
   ```
   http://localhost:7860
   ```

## ğŸ¯ How It Works

1. **User enters a prompt** in the web interface
2. **Frontend** (app.js) sends a POST request to local Flask server
3. **Flask server** (app.py) proxies the request to Colab via ngrok URL
4. **Colab** processes the prompt using the Mistral model on GPU
5. **Response flows back**: Colab â†’ Flask â†’ Browser
6. **Generated text** is displayed in the UI

## ğŸ”Œ API Endpoints

### Local Flask Server (http://localhost:7860)

#### `GET /`
Serves the frontend web interface

#### `GET /api/health`
Check if the server is running and Colab API is configured

**Response:**
```json
{
  "status": "ok",
  "message": "Frontend server is running",
  "colab_api_configured": true
}
```

#### `POST /api/generate`
Generate text using the model (proxies to Colab)

**Request:**
```json
{
  "prompt": "Write a short story about a robot",
  "max_new_tokens": 256,
  "temperature": 0.7,
  "top_p": 0.9
}
```

**Response:**
```json
{
  "generated_text": "Once upon a time, there was a robot..."
}
```

### Colab API Server (via ngrok)

The Colab notebook runs a Flask server that exposes:

#### `POST /generate`
Direct model inference endpoint

## âš™ï¸ Configuration

### Environment Variables (.env)

| Variable | Description | Example |
|----------|-------------|---------|
| `API_URL` | ngrok URL from Colab | `https://xxxx.ngrok-free.app` |
| `PORT` | Local server port | `7860` |

### Model Parameters

Adjust these in the web interface:

- **Max Tokens**: 50-512 (how much text to generate)
- **Temperature**: 0.1-2.0 (creativity level)
- **Top P**: 0.1-1.0 (nucleus sampling)

## ğŸ› ï¸ Troubleshooting

### "Cannot connect to Colab API"
- âœ… Check if Colab notebook is running
- âœ… Verify ngrok URL is correct in `.env`
- âœ… Make sure ngrok cell in Colab has run successfully

### "API_URL not configured"
- âœ… Create `.env` file in project root
- âœ… Add `API_URL=your_ngrok_url`

### "Request timeout"
- âœ… Model is loading (first request takes longer)
- âœ… Colab GPU might be slow
- âœ… Increase timeout in `app.py` if needed

### Frontend not loading
- âœ… Check if Flask server is running
- âœ… Visit `http://localhost:7860` (not file://)
- âœ… Check browser console for errors

## ğŸ“ Notes

- **Colab session expires** after inactivity (~12 hours on free tier)
- **ngrok URL changes** each time you restart the Colab notebook
- **Update `.env`** with the new ngrok URL after restarting
- **Free Colab** has usage limits - consider Colab Pro for heavy use
- **Model:** [`KASHH-4/Mistral-Model-Legal-Advisor`](https://huggingface.co/KASHH-4/Mistral-Model-Legal-Advisor) loaded with 4-bit quantization

## ğŸ¨ Frontend Features

- âœ¨ Modern, responsive UI
- âš™ï¸ Advanced settings (temperature, tokens, top_p)
- ğŸ¯ Real-time status updates
- ğŸ“Š Loading indicators
- âŒ Error handling with clear messages
- ğŸ”„ Ctrl+Enter to generate

## ğŸ” Security Notes

- Keep your **Hugging Face token** private (never commit to git)
- Keep your **ngrok token** private (never commit to git)
- Don't commit `.env` file to git (already in `.gitignore`)
- ngrok URLs are public but temporary
- **Important:** The `.env` file contains sensitive API URLs and should never be shared or committed to version control

## ğŸ“„ License

This project is licensed under a Custom Software License Agreement. See the [LICENSE](LICENSE) file for details.

**Summary:**
- âœ… Free for personal and educational use
- âœ… Study and learn from the code
- âŒ No commercial use without permission
- âŒ No redistribution

**Legal Disclaimer:** This software is for educational purposes only and does not constitute legal advice. Consult a qualified attorney for legal matters.

## ğŸ“¦ Dependencies

- **Flask**: Web server framework
- **Flask-CORS**: Cross-origin resource sharing
- **requests**: HTTP library for API calls
- **python-dotenv**: Environment variable management

## ğŸš€ Production Tips

For production deployment:
1. Use a permanent API endpoint (not ngrok)
2. Add authentication
3. Implement rate limiting
4. Set up proper logging
5. Use a production WSGI server (gunicorn)
6. Add request queuing for multiple users

---

**Model by:** KASHH-4 (organisation)  
**Model:** [`KASHH-4/Mistral-Model-Legal-Advisor`](https://huggingface.co/KASHH-4/Mistral-Model-Legal-Advisor)  
**Platform:** Google Colab + Local Flask

